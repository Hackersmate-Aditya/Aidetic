{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "from kafka import KafkaConsumer\nimport json\nfrom pymongo import MongoClient\n\n# Kafka configuration\nKAFKA_BROKER = 'localhost:9092'\nKAFKA_TOPIC = 'clickstream_topic'\n\n# MongoDB configuration\nMONGO_URI = 'mongodb://localhost:27017/'\nMONGO_DB = 'clickstream_db'\nMONGO_COLLECTION = 'clickstream_collection'\n\n# Connect to MongoDB\nclient = MongoClient(MONGO_URI)\ndb = client[MONGO_DB]\ncollection = db[MONGO_COLLECTION]\n\n# Function to ingest data from Kafka and store it in MongoDB\ndef ingest_to_mongodb():\n    consumer = KafkaConsumer(KAFKA_TOPIC, bootstrap_servers=[KAFKA_BROKER], value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n    \n    for message in consumer:\n        data = message.value\n        click_data = {\n            'click_id': data['click_id'],\n            'user_id': data['user_id'],\n            'timestamp': data['timestamp'],\n            'url': data['url'],\n            'country': data['country'],\n            'city': data['city'],\n            'browser': data['browser'],\n            'os': data['os'],\n            'device': data['device']\n        }\n        \n        # Insert data into MongoDB\n        collection.insert_one(click_data)\n        print(f\"Data ingested and stored for click_id: {data['click_id']}\")\n\n# Call the function to start ingesting data from Kafka and store it in MongoDB\ningest_to_mongodb()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, avg, countDistinct\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName('ClickstreamDataProcessor') \\\n    .getOrCreate()\n\n# MongoDB configuration (same as above)\nMONGO_URI = 'mongodb://localhost:27017/'\nMONGO_DB = 'clickstream_db'\nMONGO_COLLECTION = 'clickstream_collection'\n\n# Function to process clickstream data using Spark\ndef process_clickstream_data():\n    # Read data from MongoDB into a Spark DataFrame\n    spark_uri = f\"mongodb://127.0.0.1/{MONGO_DB}.{MONGO_COLLECTION}\"\n    clickstream_data = spark.read \\\n        .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n        .option(\"uri\", spark_uri) \\\n        .load()\n\n    # Perform aggregations by URL and country\n    processed_data = clickstream_data.groupBy(\"url\", \"country\").agg(\n        avg(col(\"timestamp\")).alias(\"average_time_spent\"),\n        count(\"timestamp\").alias(\"click_count\"),\n        countDistinct(\"user_id\").alias(\"unique_users_count\")\n    )\n\n    return processed_data\n\n# Call the function to process clickstream data and get the result DataFrame\nprocessed_data = process_clickstream_data()\nprocessed_data.show()\n\n# Stop the Spark session\nspark.stop()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk\n\n# Elasticsearch configuration\nELASTICSEARCH_HOST = 'localhost'\nELASTICSEARCH_PORT = 9200\nELASTICSEARCH_INDEX = 'processed_clickstream_data'\n\n# Function to index the processed data into Elasticsearch\ndef index_to_elasticsearch(processed_data):\n    es = Elasticsearch([{'host': ELASTICSEARCH_HOST, 'port': ELASTICSEARCH_PORT}])\n\n    # Convert the DataFrame to a list of Elasticsearch documents\n    docs = processed_data.rdd.map(lambda row: {\n        \"url\": row.url,\n        \"country\": row.country,\n        \"average_time_spent\": row.average_time_spent,\n        \"click_count\": row.click_count,\n        \"unique_users_count\": row.unique_users_count\n    }).collect()\n\n    # Bulk index the documents into Elasticsearch\n    bulk_data = [\n        {\n            \"_index\": ELASTICSEARCH_INDEX,\n            \"_source\": doc\n        }\n        for doc in docs\n    ]\n\n    bulk(es, bulk_data)\n    print(\"Data indexed in Elasticsearch.\")\n\n# Call the function to index the processed data\nindex_to_elasticsearch(processed_data)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This code outlines the entire data pipeline, including data ingestion from Kafka, storing the ingested data in MongoDB, \nprocessing the data with Apache Spark, aggregating the data, and optionally indexing the processed data into Elasticsearch.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "using MongoDB as the data store, indexing the processed data in Elasticsearch might not be necessary",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}